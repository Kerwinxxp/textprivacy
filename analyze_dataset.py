# -*- coding: utf-8 -*-
"""
åˆ†æ WikiActors æ•°æ®é›†
åŒ…æ‹¬æ•°æ®è¡Œæ•°ã€åˆ—æ•°ã€æ•°æ®ç±»å‹ã€å‰ä¸‰è¡Œè¯¦ç»†ä¿¡æ¯ç­‰
"""

import json
import pandas as pd
from typing import Dict, List, Any
import os


def load_json_dataset(file_path: str) -> List[Dict[str, Any]]:
    """
    åŠ è½½ JSON æ ¼å¼çš„æ•°æ®é›†
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data


def analyze_dataset(file_path: str):
    """
    åˆ†ææ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯
    """
    print("=" * 100)
    print("ğŸ“Š WikiActors æ•°æ®é›†åˆ†æ")
    print("=" * 100)
    
    # Step 1: åŠ è½½æ•°æ®
    print("\n1ï¸âƒ£  åŠ è½½æ•°æ®...")
    data = load_json_dataset(file_path)
    
    # Step 2: åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    print("\n2ï¸âƒ£  åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯:")
    print("-" * 100)
    
    num_rows = len(data)
    print(f"   ğŸ“ˆ æ€»è¡Œæ•°: {num_rows}")
    
    # è·å–æ‰€æœ‰åˆ—
    if num_rows > 0:
        columns = list(data[0].keys())
        num_cols = len(columns)
        print(f"   ğŸ“‹ æ€»åˆ—æ•°: {num_cols}")
        print(f"   ğŸ“ åˆ—å: {columns}")
    else:
        print("   âš ï¸  æ•°æ®é›†ä¸ºç©ºï¼")
        return
    
    # Step 3: æ•°æ®ç±»å‹å’Œç»Ÿè®¡
    print("\n3ï¸âƒ£  åˆ—çš„è¯¦ç»†ä¿¡æ¯:")
    print("-" * 100)
    
    for col_idx, col_name in enumerate(columns, 1):
        print(f"\n   [{col_idx}] åˆ—å: {col_name}")
        
        # æ”¶é›†è¯¥åˆ—çš„æ‰€æœ‰å€¼
        col_values = [row.get(col_name) for row in data]
        
        # æ•°æ®ç±»å‹
        types = set(type(val).__name__ for val in col_values if val is not None)
        print(f"       æ•°æ®ç±»å‹: {types}")
        
        # éç©ºå€¼ç»Ÿè®¡
        non_null = sum(1 for val in col_values if val is not None)
        null_count = num_rows - non_null
        print(f"       éç©ºå€¼: {non_null} / {num_rows}")
        if null_count > 0:
            print(f"       ç©ºå€¼: {null_count}")
        
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç»Ÿè®¡é•¿åº¦
        if types == {'str'}:
            lengths = [len(str(val)) for val in col_values if val is not None]
            if lengths:
                print(f"       å­—ç¬¦ä¸²é•¿åº¦ - æœ€å°: {min(lengths)}, æœ€å¤§: {max(lengths)}, å¹³å‡: {sum(lengths)/len(lengths):.1f}")
        
        # å¦‚æœæ˜¯æ•°å­—ï¼Œæ˜¾ç¤ºèŒƒå›´
        if types in [{'int'}, {'float'}, {'int', 'float'}]:
            numeric_vals = [val for val in col_values if isinstance(val, (int, float)) and val is not None]
            if numeric_vals:
                print(f"       æ•°å€¼èŒƒå›´ - æœ€å°: {min(numeric_vals)}, æœ€å¤§: {max(numeric_vals)}, å¹³å‡: {sum(numeric_vals)/len(numeric_vals):.2f}")
    
    # Step 4: å‰ä¸‰è¡Œçš„è¯¦ç»†ä¿¡æ¯
    print("\n" + "=" * 100)
    print("4ï¸âƒ£  å‰ä¸‰è¡Œçš„è¯¦ç»†ä¿¡æ¯:")
    print("=" * 100)
    
    num_rows_to_show = min(3, num_rows)
    
    for row_idx in range(num_rows_to_show):
        print(f"\nğŸ“„ ç¬¬ {row_idx + 1} è¡Œ:")
        print("-" * 100)
        
        row = data[row_idx]
        
        for col_idx, col_name in enumerate(columns, 1):
            value = row.get(col_name)
            
            # æ ¼å¼åŒ–è¾“å‡º
            if isinstance(value, str):
                # å¦‚æœå­—ç¬¦ä¸²å¤ªé•¿ï¼Œæˆªæ–­
                if len(value) > 150:
                    print(f"   [{col_idx}] {col_name}:")
                    print(f"       {value[:150]}...")
                    print(f"       (å®Œæ•´é•¿åº¦: {len(value)} å­—ç¬¦)")
                else:
                    print(f"   [{col_idx}] {col_name}: {value}")
            elif isinstance(value, list):
                print(f"   [{col_idx}] {col_name} (åˆ—è¡¨ï¼Œ{len(value)} é¡¹):")
                # æ˜¾ç¤ºåˆ—è¡¨çš„å‰ 3 é¡¹
                for item_idx, item in enumerate(value[:3]):
                    print(f"       [{item_idx}] {item}")
                if len(value) > 3:
                    print(f"       ... è¿˜æœ‰ {len(value) - 3} é¡¹")
            elif isinstance(value, dict):
                print(f"   [{col_idx}] {col_name} (å­—å…¸ï¼Œ{len(value)} é¡¹):")
                # æ˜¾ç¤ºå­—å…¸çš„å‰ 3 é¡¹
                for key_idx, (key, val) in enumerate(list(value.items())[:3]):
                    print(f"       {key}: {val}")
                if len(value) > 3:
                    print(f"       ... è¿˜æœ‰ {len(value) - 3} é¡¹")
            else:
                print(f"   [{col_idx}] {col_name}: {value}")
    
    # Step 5: ç”Ÿæˆ DataFrame æ‘˜è¦ï¼ˆå¯é€‰ï¼‰
    print("\n" + "=" * 100)
    print("5ï¸âƒ£  è½¬æ¢ä¸º Pandas DataFrame:")
    print("-" * 100)
    
    try:
        df = pd.DataFrame(data)
        print(f"\n   DataFrame å½¢çŠ¶: {df.shape}")
        print(f"\n   æ•°æ®ç±»å‹:\n{df.dtypes}")
        print(f"\n   ç¼ºå¤±å€¼ç»Ÿè®¡:\n{df.isnull().sum()}")
        print(f"\n   æ•°å€¼åˆ—ç»Ÿè®¡:\n{df.describe()}")
    except Exception as e:
        print(f"   âš ï¸  æ— æ³•è½¬æ¢ä¸º DataFrame: {e}")
    
    print("\n" + "=" * 100)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("=" * 100)


def save_analysis_report(file_path: str, output_file: str = None):
    """
    ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°æ–‡ä»¶
    """
    if output_file is None:
        output_file = file_path.replace('.json', '_analysis_report.txt')
    
    # é‡å®šå‘è¾“å‡ºåˆ°æ–‡ä»¶
    import sys
    from io import StringIO
    
    old_stdout = sys.stdout
    sys.stdout = StringIO()
    
    try:
        analyze_dataset(file_path)
        report_content = sys.stdout.getvalue()
    finally:
        sys.stdout = old_stdout
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    print(report_content)


if __name__ == "__main__":
    # æŒ‡å®šæ•°æ®æ–‡ä»¶è·¯å¾„
    data_file = r"C:\Users\phdwf\Desktop\textreidentify\TextReIdentification\data\WikiActors_2000_filtered.json"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        exit(1)
    
    # åˆ†ææ•°æ®é›†
    analyze_dataset(data_file)
    
    # å¯é€‰ï¼šä¿å­˜æŠ¥å‘Š
    # save_analysis_report(data_file)