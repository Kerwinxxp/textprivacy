import pandas as pd
import spacy
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel
import re
from typing import List, Dict, Any, Tuple
import time
from tqdm import tqdm

# åŠ è½½spaCy NERæ¨¡å‹ï¼ˆç”¨äºå®ä½“è¯†åˆ«ï¼‰
nlp = spacy.load("en_core_web_lg")

# åŠ è½½Transformeræ¨¡å‹ï¼ˆå‚è€ƒTRIä»£ç ä¸­çš„DistilBERTï¼‰
model_name = "distilbert-base-uncased"  # æˆ–ä»config.jsonä¸­è¯»å–
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

# æ•°æ®æ–‡ä»¶è·¯å¾„
data_file = r"c:\Users\phdwf\Desktop\textreidentify\TextReIdentification\data\WikiActors_50_eval.json"
output_file = r"c:\Users\phdwf\Desktop\textreidentify\TextReIdentification\data\WikiActors_50_eval_with_noisy_public_knowledge.json"

# åŠ è½½æ•°æ®
df = pd.read_json(data_file)

class SimpleExponentialMechanism:
    """
    ç®€åŒ–çš„æŒ‡æ•°æœºåˆ¶ï¼Œç”¨äºtokenåŠ å™ªï¼ˆåŸºäºembeddingè·ç¦»ï¼‰
    """
    def __init__(self, model, tokenizer, epsilon=1.0, candidate_pool_size=500):
        self.model = model
        self.tokenizer = tokenizer
        self.epsilon = epsilon
        self.candidate_pool_size = candidate_pool_size
        self.vocab_size = tokenizer.vocab_size
        
        # é¢„åŠ è½½æ‰€æœ‰token embeddingï¼ˆä¼˜åŒ–ï¼‰
        with torch.no_grad():
            all_token_ids = torch.arange(self.vocab_size)
            self.all_embeddings = model.get_input_embeddings()(all_token_ids).detach().cpu().numpy()
    
    def get_token_embedding(self, token: str) -> np.ndarray:
        """è·å–tokençš„embedding"""
        inputs = tokenizer(token, return_tensors="pt", add_special_tokens=False)
        with torch.no_grad():
            outputs = model(**inputs)
            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()
        return embedding
    
    def select_noisy_token(self, original_embedding: np.ndarray) -> Tuple[str, np.ndarray]:
        """ä½¿ç”¨æŒ‡æ•°æœºåˆ¶é€‰æ‹©å™ªå£°tokenï¼Œå¹¶è¿”å›æ¦‚ç‡åˆ†å¸ƒ"""
        # éšæœºé€‰æ‹©å€™é€‰æ± 
        candidate_ids = np.random.choice(self.vocab_size, size=self.candidate_pool_size, replace=False)
        candidate_embeddings = self.all_embeddings[candidate_ids]
        
        # è®¡ç®—è·ç¦»
        distances = np.linalg.norm(candidate_embeddings - original_embedding, axis=1)
        
        # æŒ‡æ•°æœºåˆ¶ï¼šè®¡ç®—æ¦‚ç‡
        scaled_distances = -(self.epsilon / 2) * distances
        exp_values = np.exp(scaled_distances - np.max(scaled_distances))
        probabilities = exp_values / np.sum(exp_values)
        
        # é‡‡æ ·
        selected_idx = np.random.choice(len(candidate_ids), p=probabilities)
        selected_token_id = candidate_ids[selected_idx]
        selected_token = tokenizer.decode([selected_token_id])
        
        return selected_token, probabilities

# åˆå§‹åŒ–æŒ‡æ•°æœºåˆ¶
mechanism = SimpleExponentialMechanism(model, tokenizer, epsilon=1.0)  # Îµå¯è°ƒ

# å…¨å±€è®¡æ•°å™¨ï¼Œç”¨äºæ‰“å°å‰3ä¸ªå®ä½“
entity_count = 0

# å¤„ç†æ¯ä¸€è¡Œï¼Œæ·»åŠ noisy_public_knowledge
def process_row(row):
    global entity_count
    text = row['public_knowledge']  # å¯¹background_knowledge_columnåŠ å™ª
    doc = nlp(text)
    new_text = text
    
    # è¯†åˆ«å®ä½“å¹¶æ›¿æ¢
    for ent in doc.ents:
        entity_text = ent.text
        
        # å¯¹å®ä½“è¿›è¡Œtokenizeï¼ˆæŒ‰tokenåŠ å™ªï¼‰
        tokens = tokenizer.tokenize(entity_text)
        noisy_tokens = []
        probabilities_list = []
        
        for token in tokens:
            token_embedding = mechanism.get_token_embedding(token)
            noisy_token, probabilities = mechanism.select_noisy_token(token_embedding)
            noisy_tokens.append(noisy_token)
            probabilities_list.append(probabilities)
        
        # é‡æ–°ç»„åˆå™ªå£°token
        noisy_entity = tokenizer.convert_tokens_to_string(noisy_tokens)
        
        # æ‰“å°å‰3ä¸ªå®ä½“çš„ä¿¡æ¯
        if entity_count < 3:
            print(f"\nğŸ” Entity {entity_count + 1}:")
            print(f"  Original entity: '{entity_text}'")
            print(f"  Tokens: {tokens}")
            print(f"  Noisy entity: '{noisy_entity}'")
            print(f"  Noisy tokens: {noisy_tokens}")
            if probabilities_list:
                print(f"  Probability distribution for first token (first 10): {probabilities_list[0][:10]}")
            entity_count += 1
        
        # ç²¾ç¡®æ›¿æ¢å®ä½“
        new_text = re.sub(r'\b' + re.escape(entity_text) + r'\b', re.escape(noisy_entity), new_text)
    
    return new_text

# è®°å½•å¼€å§‹æ—¶é—´
start_time = time.time()

# åº”ç”¨å¤„ç†ï¼ˆä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦ï¼‰
print("å¼€å§‹åŠ å™ªå¤„ç†...")
noisy_list = []
for i, row in tqdm(df.iterrows(), total=len(df), desc="Processing rows"):
    noisy_text = process_row(row)
    noisy_list.append(noisy_text)

df['noisy_public_knowledge'] = noisy_list

# è®°å½•ç»“æŸæ—¶é—´
end_time = time.time()
total_time = end_time - start_time

# ä¿å­˜æ›´æ–°åçš„æ•°æ®
df.to_json(output_file, orient='records')
print(f"æ·»åŠ  noisy_public_knowledge åˆ—å®Œæˆï¼Œä¿å­˜åˆ° {output_file}")
print(f"æ€»å¤„ç†æ—¶é—´: {total_time:.2f} ç§’")

# æ‰“å°ç¬¬ä¸€è¡Œçš„å¯¹æ¯”
print("\nç¬¬ä¸€è¡Œæ•°æ®å¯¹æ¯”ï¼š")
print(f"  Name: {df.loc[0, 'name']}")
print(f"  Original Public Knowledge: {df.loc[0, 'public_knowledge']}")
print(f"  Noisy Public Knowledge: {df.loc[0, 'noisy_public_knowledge']}")