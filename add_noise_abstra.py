import pandas as pd
import spacy
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel
import re
import random
from typing import List, Dict, Any, Tuple

# 1. è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯å¤ç°
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(42)

# 2. åŠ è½½spaCy NERæ¨¡å‹ï¼ˆç”¨äºå®ä½“è¯†åˆ«ï¼‰
print("æ­£åœ¨åŠ è½½ spaCy æ¨¡å‹...")
nlp = spacy.load("en_core_web_lg")

# 3. åŠ è½½Transformeræ¨¡å‹
print("æ­£åœ¨åŠ è½½ DistilBERT æ¨¡å‹...")
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")
model.to(device)
model.eval()

# è®¾ç½®æ¯è¡Œæ•°æ®çš„æ€»éšç§é¢„ç®—
TOTAL_EPSILON_BUDGET = 30
# é¢„ç®—åˆ†é…ç­–ç•¥: 'shared' (å‡åˆ†) æˆ– 'independent' (ç‹¬ç«‹)
BUDGET_ALLOCATION_STRATEGY = 'independent' 

# æ•°æ®æ–‡ä»¶è·¯å¾„
data_file = r"C:\Users\phdwf\OneDrive\Desktop\textreidentify\TextReIdentification\data\WikiActors_50_masked_cleaned.json"
# åŠ¨æ€ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
output_file = fr"C:\Users\phdwf\OneDrive\Desktop\textreidentify\TextReIdentification\data\noise_budget_{TOTAL_EPSILON_BUDGET}_{BUDGET_ALLOCATION_STRATEGY}.json"

# åŠ è½½æ•°æ®
df = pd.read_json(data_file)

class SimpleExponentialMechanism:
    """
    æ”¹è¿›çš„æŒ‡æ•°æœºåˆ¶ï¼šä½¿ç”¨ Top-K æœ€è¿‘é‚»ä½œä¸ºå€™é€‰æ± 
    """
    def __init__(self, model, tokenizer, candidate_pool_size=50):
        self.model = model
        self.tokenizer = tokenizer
        # self.epsilon = epsilon  <-- ç§»é™¤å›ºå®šçš„ epsilon
        self.candidate_pool_size = candidate_pool_size
        self.vocab_size = tokenizer.vocab_size
        self.device = model.device
        
        print("æ­£åœ¨é¢„åŠ è½½å…¨è¯è¡¨ Embeddings...")
        # é¢„åŠ è½½æ‰€æœ‰token embeddingï¼Œå¹¶è½¬ä¸º Tensor æ”¾åœ¨è®¾å¤‡ä¸Šä»¥åŠ é€Ÿè®¡ç®—
        with torch.no_grad():
            all_token_ids = torch.arange(self.vocab_size).to(self.device)
            # è·å–é™æ€ embedding table
            self.all_embeddings = model.get_input_embeddings()(all_token_ids)
    
    # def get_token_embedding(self, token: str) -> torch.Tensor:
    #     """è·å–tokençš„embedding (è¿”å› Tensor)"""
    #     inputs = tokenizer(token, return_tensors="pt", add_special_tokens=False).to(self.device)
    #     with torch.no_grad():
    #         outputs = model(**inputs)
    #         # è·å– embedding (å–å¹³å‡æˆ–ç›´æ¥å–ç¬¬ä¸€ä¸ªtoken)
    #         embedding = outputs.last_hidden_state.mean(dim=1).squeeze()
    #     return embedding
    def get_token_embedding(self, token: str) -> torch.Tensor:
        """
        ä¿®æ­£ç‰ˆï¼šç›´æ¥è·å– Input Embeddingï¼Œè€Œä¸æ˜¯æ¨¡å‹è¾“å‡ºçš„ Hidden State
        """
        # 1. å°† token string (å¦‚ "##lie") è½¬å› token id
        # æ³¨æ„ï¼šè¿™é‡Œç›´æ¥ä½¿ç”¨ convert_tokens_to_idsï¼Œå®ƒèƒ½æ­£ç¡®å¤„ç† "##" å‰ç¼€
        token_id = self.tokenizer.convert_tokens_to_ids(token)
        
        # å¤„ç†æœªçŸ¥è¯çš„æƒ…å†µï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰
        if token_id == self.tokenizer.unk_token_id and token != self.tokenizer.unk_token:
            print(f"Warning: Token '{token}' unknown to tokenizer.")
            
        # 2. è½¬ä¸º Tensor
        token_id_tensor = torch.tensor(token_id).to(self.device)
        
        # 3. ç›´æ¥ä»é¢„åŠ è½½çš„ all_embeddings ä¸­æŸ¥è¡¨
        # æˆ–è€…ä½¿ç”¨ self.model.get_input_embeddings()(token_id_tensor)
        # è¿™é‡Œä¸ºäº†åˆ©ç”¨ä½ å·²ç»åŠ è½½çš„ self.all_embeddingsï¼Œç›´æ¥ç´¢å¼•å³å¯
        embedding = self.all_embeddings[token_id_tensor]
        
        return embedding
    def select_noisy_token(self, original_embedding: torch.Tensor, epsilon: float) -> Tuple[str, np.ndarray]:
        """
        1. è®¡ç®—åŸè¯ä¸å…¨è¯è¡¨çš„è·ç¦»
        2. é€‰å‡º Top-K ä¸ªæœ€è¿‘çš„è¯ä½œä¸ºå€™é€‰æ± 
        3. åœ¨è¿™ K ä¸ªè¯ä¸­åº”ç”¨æŒ‡æ•°æœºåˆ¶
        """
        # 1. è®¡ç®—ä¸æ‰€æœ‰è¯çš„æ¬§æ°è·ç¦» (åˆ©ç”¨ PyTorch å¹¿æ’­æœºåˆ¶åŠ é€Ÿ)
        # diff shape: [vocab_size, hidden_dim]
        diff = self.all_embeddings - original_embedding
        # distances shape: [vocab_size]
        distances = torch.norm(diff, p=2, dim=1)
        
        # 2. ç­›é€‰ Top-K (é€‰å‡ºè·ç¦»æœ€å°çš„ candidate_pool_size ä¸ª)
        # values æ˜¯è·ç¦»ï¼Œindices æ˜¯ token_id
        topk_distances, topk_indices = torch.topk(distances, self.candidate_pool_size, largest=False)
        
        # è½¬å› numpy è¿›è¡Œæ¦‚ç‡è®¡ç®—
        candidate_ids = topk_indices.cpu().numpy()
        candidate_distances = topk_distances.cpu().numpy()
        
        # 3. æŒ‡æ•°æœºåˆ¶ï¼šè®¡ç®—æ¦‚ç‡
        # score = -distance (è·ç¦»è¶Šå°ï¼Œåˆ†æ•°è¶Šé«˜)
        # P(t) = exp(epsilon * score / 2)
        # ä½¿ç”¨ä¼ å…¥çš„åŠ¨æ€ epsilon
        scaled_scores = -(epsilon / 2) * candidate_distances
        
        # æ•°å€¼ç¨³å®šæ€§å¤„ç†ï¼šå‡å»æœ€å¤§å€¼é˜²æ­¢ exp æº¢å‡º
        exp_values = np.exp(scaled_scores - np.max(scaled_scores))
        probabilities = exp_values / np.sum(exp_values)
        
        # 4. é‡‡æ ·
        selected_idx = np.random.choice(len(candidate_ids), p=probabilities)
        selected_token_id = candidate_ids[selected_idx]
        
        # è§£ç 
        selected_token = tokenizer.decode([selected_token_id])
        
        return selected_token, probabilities

# åˆå§‹åŒ–æŒ‡æ•°æœºåˆ¶ (ä¸å†ä¼ å…¥å›ºå®šçš„ epsilon)
mechanism = SimpleExponentialMechanism(model, tokenizer, candidate_pool_size=50)

# å…¨å±€è®¡æ•°å™¨ï¼Œç”¨äºæ‰“å°å‰3ä¸ªå®ä½“
entity_count = 0

# å¤„ç†æ¯ä¸€è¡Œï¼Œæ·»åŠ noise_abstract
def process_row(row):
    global entity_count
    text = row['original_abstract']
    doc = nlp(text)
    new_text = text

    # --- ç»Ÿè®¡é€»è¾‘ ---
    all_entity_texts = [ent.text for ent in doc.ents]
    total_ent_count = len(all_entity_texts)
    unique_ent_count = len(set(all_entity_texts))
    
    # --- åŠ¨æ€é¢„ç®—åˆ†é… ---
    if BUDGET_ALLOCATION_STRATEGY == 'shared':
        # å¦‚æœæœ‰å®ä½“ï¼Œå°†æ€»é¢„ç®—å‡åˆ†ç»™æ¯ä¸ªå®ä½“
        if total_ent_count > 0:
            current_epsilon = TOTAL_EPSILON_BUDGET / total_ent_count
        else:
            current_epsilon = TOTAL_EPSILON_BUDGET 
    elif BUDGET_ALLOCATION_STRATEGY == 'independent':
        # æ¯ä¸ªå®ä½“ç‹¬ç«‹åŠ å™ªåˆ†é…ï¼Œä½¿ç”¨å®Œæ•´é¢„ç®—
        current_epsilon = TOTAL_EPSILON_BUDGET
    else:
        raise ValueError(f"Unknown strategy: {BUDGET_ALLOCATION_STRATEGY}")
    # ----------------

    # è¯†åˆ«å®ä½“å¹¶æ›¿æ¢
    for ent in doc.ents:
        entity_text = ent.text
        
        # å¯¹å®ä½“è¿›è¡Œtokenizeï¼ˆæŒ‰tokenåŠ å™ªï¼‰
        tokens = tokenizer.tokenize(entity_text)
        noisy_tokens = []
        probabilities_list = []
        
        for token in tokens:
            # è·å– embedding (Tensor)
            token_embedding = mechanism.get_token_embedding(token)
            # é€‰æ‹©å™ªå£°è¯ï¼Œä¼ å…¥è®¡ç®—å¥½çš„ current_epsilon
            noisy_token, probabilities = mechanism.select_noisy_token(token_embedding, epsilon=current_epsilon)
            
            # æ¸…ç† token (å»é™¤ BERT çš„ subword å‰ç¼€ '##')
            clean_noisy_token = noisy_token.replace("##", "")
            noisy_tokens.append(clean_noisy_token)
            probabilities_list.append(probabilities)
        
        # é‡æ–°ç»„åˆå™ªå£°token
        noisy_entity = " ".join(noisy_tokens)
        noisy_entity = noisy_entity.replace(" .", ".").replace(" ,", ",")
        
        # æ‰“å°å‰3ä¸ªå®ä½“çš„ä¿¡æ¯
        if entity_count < 3:
            print(f"\nğŸ” Entity {entity_count + 1}:")
            print(f"  Original entity: '{entity_text}'")
            print(f"  Budget: {current_epsilon:.4f} (Total: {TOTAL_EPSILON_BUDGET} / {total_ent_count} entities)")
            print(f"  Tokens: {tokens}")
            print(f"  Noisy entity: '{noisy_entity}'")
            if probabilities_list:
                max_prob_idx = np.argmax(probabilities_list[0])
                print(f"  Max prob for 1st token: {probabilities_list[0][max_prob_idx]:.4f}")
            entity_count += 1
        
        # æ›¿æ¢é€»è¾‘
        new_text = re.sub(r'\b' + re.escape(entity_text) + r'\b', noisy_entity, new_text)
    
    # è¿”å›ä¸‰ä¸ªå€¼
    return new_text, total_ent_count, unique_ent_count

print("å¼€å§‹å¤„ç†æ•°æ®...")
# åº”ç”¨å¤„ç†ï¼Œresult_type='expand' å°†å…ƒç»„æ‹†åˆ†ä¸ºå¤šåˆ—
df[['noise_abstract', 'entity_count', 'unique_entity_count']] = df.apply(process_row, axis=1, result_type='expand')

# ä¿å­˜æ›´æ–°åçš„æ•°æ®
df.to_json(output_file, orient='records')
print(f"å¤„ç†å®Œæˆï¼Œä¿å­˜åˆ° {output_file}")

# æ‰“å°ç»Ÿè®¡ä¿¡æ¯
print("\n=== ç»Ÿè®¡æŠ¥å‘Š ===")
print(f"å¹³å‡æ¯è¡Œå®ä½“æ•° (Total Entities): {df['entity_count'].mean():.2f}")
print(f"å¹³å‡æ¯è¡Œå”¯ä¸€å®ä½“æ•° (Unique Entities): {df['unique_entity_count'].mean():.2f}")
print(f"æœ€å¤šå®ä½“çš„è¡ŒåŒ…å«: {df['entity_count'].max()} ä¸ªå®ä½“")
print(f"æœ€å°‘å®ä½“çš„è¡ŒåŒ…å«: {df['entity_count'].min()} ä¸ªå®ä½“")

# æ‰“å°ç¬¬ä¸€è¡Œçš„å¯¹æ¯”
print("\nç¬¬ä¸€è¡Œæ•°æ®å¯¹æ¯”ï¼š")
if not df.empty:
    print(f"  Name: {df.loc[0, 'name']}")
    print(f"  Entity Count: {df.loc[0, 'entity_count']}")
    print(f"  Unique Entity Count: {df.loc[0, 'unique_entity_count']}")
    print(f"  Original Abstract: {df.loc[0, 'original_abstract']}")
    print(f"  Noise Abstract: {df.loc[0, 'noise_abstract']}")