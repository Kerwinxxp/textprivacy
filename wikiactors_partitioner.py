# -*- coding: utf-8 -*-
"""
WikiActors æ•°æ®é›† PII åˆ†ç»„ Partition
ä¸¥æ ¼æŒ‰ç…§ news_pii_partition_pipeline.py çš„æ ¼å¼å¤„ç†
"""

import json
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
from tqdm import tqdm

# å¯¼å…¥ä¼˜åŒ–ç‰ˆæœ¬çš„ç®¡é“æ¨¡å—
import sys
sys.path.append('.')

from news_pii_partition_pipeline import (
    SpacyModelLoader, TextProcessor, OutputWriter, StatsCollector
)

# ===================== é…ç½® =====================
DATA_DIR = Path("./data")
INPUT_FILE = DATA_DIR / "WikiActors_50_masked_cleaned.json"
OUTPUT_FILE = Path("./Wikiactors_partition.jsonl")
OUTPUT_CSV = Path("./Wikiactors_partition.csv")
SPACY_MODEL = "en_core_web_sm"


# ===================== ä¸»ç±» =====================

class WikiActorsPartitioner:
    """WikiActors æ•°æ®é›† Partition å¤„ç†å™¨"""
    
    def __init__(self, model_name: str = SPACY_MODEL):
        """åˆå§‹åŒ–åˆ†åŒºå¤„ç†å™¨"""
        print("=" * 80)
        print("ğŸ¬ WikiActors æ•°æ®é›† PII åˆ†ç»„ Partition")
        print("=" * 80)
        
        # åŠ è½½ spaCy æ¨¡å‹
        print(f"\n1ï¸âƒ£  åŠ è½½ spaCy æ¨¡å‹: {model_name}")
        self.nlp = SpacyModelLoader.load(model_name)
        SpacyModelLoader.enable_senter(self.nlp)
        self.coref_enabled = SpacyModelLoader.enable_coref(self.nlp)
        print(f"   å…±æŒ‡æ¶ˆè§£: {'âœ… å¯ç”¨' if self.coref_enabled else 'âš ï¸ æœªå¯ç”¨'}")
    
    def load_wikiactors(self, json_file: Path) -> List[Dict[str, Any]]:
        """åŠ è½½ WikiActors JSON æ•°æ®"""
        try:
            print(f"\n   ğŸ“‚ æ•°æ®è·¯å¾„: {json_file.absolute()}")
            
            if not json_file.exists():
                print(f"   âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
                return []
            
            with open(json_file, 'r', encoding='utf-8') as f:
                records = json.load(f)
            
            print(f"   âœ… åŠ è½½æˆåŠŸï¼š{len(records)} ä¸ªæ¼”å‘˜æ¡£æ¡ˆ")
            return records
        except Exception as e:
            print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
            return []
    
    def process_dataset(self, json_file: Path) -> tuple:
        """å¤„ç†æ•´ä¸ª WikiActors æ•°æ®é›† - è¿”å› JSONL è®°å½•å’Œç»Ÿè®¡ä¿¡æ¯"""
        records = self.load_wikiactors(json_file)
        
        if not records:
            print("âŒ æ²¡æœ‰æ•°æ®å¯å¤„ç†")
            return [], []
        
        print(f"\n2ï¸âƒ£  å¤„ç† {len(records)} ä¸ªæ¼”å‘˜æ¡£æ¡ˆ...")
        
        jsonl_records = []
        stats = StatsCollector()
        
        for idx, actor_record in enumerate(tqdm(records, desc="å¤„ç†æ¼”å‘˜æ¡£æ¡ˆ")):
            name = actor_record.get('name', 'Unknown')
            abstract = actor_record.get('original_abstract', '')
            
            # è·³è¿‡ç©ºçš„æ‘˜è¦
            if not abstract or len(abstract.strip()) < 10:
                continue
            
            try:
                # ä½¿ç”¨ TextProcessor å¤„ç†æ–‡æœ¬ - ä¸¥æ ¼æŒ‰ç…§ pipeline çš„æ–¹å¼
                clusters, ner_info = TextProcessor.process(
                    self.nlp, 
                    abstract, 
                    save_ner=False,  # ä¸ä¿å­˜ NER ä¿¡æ¯
                    has_coref=self.coref_enabled
                )
                
                has_fallback = any(v.get("is_fallback", False) for v in clusters.values())
                
                # ä½¿ç”¨ OutputWriter.write_clusters æ¥ç”Ÿæˆè®°å½• - ä¿æŒæ ¼å¼ä¸€è‡´
                rec = OutputWriter.write_clusters(
                    clusters, 
                    OUTPUT_FILE, 
                    idx, 
                    abstract, 
                    has_fallback, 
                    ner_info=None  # ä¸ä¿å­˜ NER
                )
                
                # æ·»åŠ  actor ä¿¡æ¯
                rec["actor_name"] = name
                
                jsonl_records.append(rec)
                
                # æ›´æ–°ç»Ÿè®¡
                stats.update(clusters, has_fallback, num_ner=0)
                
            except Exception as e:
                print(f"   âš ï¸ å¤„ç† {name} å¤±è´¥: {e}")
                continue
        
        return jsonl_records, stats
    
    def save_results(self, jsonl_records: List[Dict]) -> None:
        """ä¿å­˜ç»“æœåˆ° JSONL å’Œ CSV"""
        if not jsonl_records:
            print("âŒ æ²¡æœ‰è®°å½•å¯ä¿å­˜")
            return
        
        # ä¿å­˜ JSONL
        try:
            with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                for record in jsonl_records:
                    f.write(json.dumps(record, ensure_ascii=False) + '\n')
            print(f"\nâœ… JSONL å·²ä¿å­˜åˆ°: {OUTPUT_FILE.absolute()}")
            print(f"   - æ€»è®°å½•æ•°: {len(jsonl_records)}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ JSONL å¤±è´¥: {e}")
        
        # ä¿å­˜ CSV - å±•å¹³ç»“æ„ä»¥ä¾¿æŸ¥çœ‹
        try:
            csv_records = []
            for rec in jsonl_records:
                csv_rec = {
                    'row_id': rec.get('row_id'),
                    'actor_name': rec.get('actor_name', ''),
                    'text_length': rec.get('text_length'),
                    'num_persons': rec.get('num_persons'),
                    'num_pii_mentions': rec.get('num_pii_mentions'),
                    'has_fallback_anchor': rec.get('has_fallback_anchor'),
                    'persons': '|'.join(rec.get('persons', [])),
                    'pii_types': self._extract_pii_types(rec)
                }
                csv_records.append(csv_rec)
            
            df = pd.DataFrame(csv_records)
            df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')
            print(f"âœ… CSV å·²ä¿å­˜åˆ°: {OUTPUT_CSV.absolute()}")
            print(f"   - æ€»è¡Œæ•°: {len(csv_records)}")
            print(f"   - æ€»åˆ—æ•°: {len(df.columns)}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ CSV å¤±è´¥: {e}")
    
    @staticmethod
    def _extract_pii_types(record: Dict) -> str:
        """ä» clusters ä¸­æå–æ‰€æœ‰ PII ç±»å‹"""
        pii_types = set()
        for cluster in record.get('clusters', {}).values():
            pii_types.update(cluster.get('pii_types', []))
        return '|'.join(sorted(pii_types))


def main():
    """ä¸»å‡½æ•°"""
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    partitioner = WikiActorsPartitioner(model_name=SPACY_MODEL)
    
    # å¤„ç†æ•°æ®é›†
    jsonl_records, stats = partitioner.process_dataset(INPUT_FILE)
    
    if jsonl_records:
        # ä¿å­˜ç»“æœ
        partitioner.save_results(jsonl_records)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\n3ï¸âƒ£  å¤„ç†ç»Ÿè®¡:")
        stats.print_summary(save_ner=False)
        
        print("\n" + "=" * 80)
        print("âœ… å¤„ç†å®Œæˆ!")
        print("=" * 80)
    else:
        print("âŒ æ²¡æœ‰ç”Ÿæˆç»“æœ")


if __name__ == "__main__":
    main()