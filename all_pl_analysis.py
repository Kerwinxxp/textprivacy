
# filepath: c:\Users\phdwf\Desktop\textreidentify\TextReIdentification\all_pl_analysis.py
# -*- coding: utf-8 -*-
"""
æ‰€æœ‰ PL vs PIInum å…³ç³»å›¾ - åŸå§‹æ•°æ®ç›´æ¥ç»˜åˆ¶
æ”¯æŒå¤šç§å¤šé¡¹å¼æ‹Ÿåˆ + å¼‚å¸¸å€¼æ’é™¤ + ä¸Šé™åŒ…ç»œçº¿
"""

import json
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from typing import Dict, List, Tuple
import os
import seaborn as sns
from sklearn.metrics import r2_score


def calculate_all_pl_pairs(
    prior_probs: np.ndarray,
    posterior_probs: np.ndarray,
    labels: np.ndarray,
    class_metric: np.ndarray = None,
    epsilon: float = 1e-12
) -> Tuple[List[float], List[int]]:
    """
    è®¡ç®—æ‰€æœ‰ç±»å¯¹çš„ PL å€¼ï¼ˆä¸èšåˆï¼‰
    """
    N, C = prior_probs.shape
    
    A = np.clip(prior_probs, epsilon, 1.0)
    B = np.clip(posterior_probs, epsilon, 1.0)
    A = A / A.sum(axis=1, keepdims=True)
    B = B / B.sum(axis=1, keepdims=True)
    
    if class_metric is None:
        D = np.ones((C, C), dtype=float)
        np.fill_diagonal(D, np.inf)
    else:
        D = np.array(class_metric, dtype=float)
        D = np.where(D <= 0, epsilon, D)
    
    all_pl_values = []
    all_sample_labels = []
    
    for s in range(N):
        la = np.log(A[s])
        lb = np.log(B[s])
        diff = np.abs((lb[:, None] - lb[None, :]) - (la[:, None] - la[None, :]))
        mask = ~np.eye(C, dtype=bool)
        normed = diff[mask] / D[mask]
        
        all_pl_values.extend(normed)
        all_sample_labels.extend([labels[s]] * len(normed))
    
    return all_pl_values, all_sample_labels


# ===================== å¼‚å¸¸å€¼æ£€æµ‹ =====================

def detect_outliers_iqr(y_values, k=1.5):
    """
    ä½¿ç”¨ IQR æ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
    k=1.5 æ˜¯æ ‡å‡†å€¼ï¼Œk=3 æ›´ä¸¥æ ¼ï¼ˆæ’é™¤æ›´å°‘çš„ç‚¹ï¼‰
    è¿”å›: å¸ƒå°”æ•°ç»„ï¼ŒTrue è¡¨ç¤ºå¼‚å¸¸å€¼
    """
    Q1 = np.percentile(y_values, 25)
    Q3 = np.percentile(y_values, 75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - k * IQR
    upper_bound = Q3 + k * IQR
    
    outliers = (y_values < lower_bound) | (y_values > upper_bound)
    
    return outliers, lower_bound, upper_bound


def detect_outliers_zscore(y_values, threshold=3):
    """
    ä½¿ç”¨ Z-Score æ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
    threshold=3 è¡¨ç¤ºè¶…è¿‡ 3 å€æ ‡å‡†å·®çš„ç‚¹
    è¿”å›: å¸ƒå°”æ•°ç»„ï¼ŒTrue è¡¨ç¤ºå¼‚å¸¸å€¼
    """
    mean = np.mean(y_values)
    std = np.std(y_values)
    
    z_scores = np.abs((y_values - mean) / std)
    outliers = z_scores > threshold
    
    return outliers


def detect_outliers_modified_zscore(y_values, threshold=3.5):
    """
    ä½¿ç”¨æ”¹è¿›çš„ Z-Score æ–¹æ³•ï¼ˆåŸºäºä¸­ä½ç»å¯¹åå·®ï¼‰
    å¯¹æç«¯å¼‚å¸¸å€¼æ›´æ•æ„Ÿ
    """
    median = np.median(y_values)
    mad = np.median(np.abs(y_values - median))
    
    modified_z_scores = 0.6745 * (y_values - median) / mad
    outliers = np.abs(modified_z_scores) > threshold
    
    return outliers


# ===================== æ‹Ÿåˆå‡½æ•° =====================

def fit_and_evaluate_polynomials(x_points, y_points, degrees=[1, 2, 3]):
    """
    ç”¨ä¸åŒé˜¶æ•°çš„å¤šé¡¹å¼æ‹Ÿåˆ + å¯¹æ•°æ‹Ÿåˆï¼Œå¹¶è®¡ç®— RÂ² åˆ†æ•°
    """
    results = {}
    
    # 1. å¤šé¡¹å¼æ‹Ÿåˆ
    for degree in degrees:
        try:
            coeffs = np.polyfit(x_points, y_points, degree)
            poly = np.poly1d(coeffs)
            y_pred = poly(x_points)
            r2 = r2_score(y_points, y_pred)
            
            results[degree] = {
                'coeffs': coeffs,
                'poly': poly,
                'r2': r2,
                'label': f'Poly (d={degree})'
            }
        except Exception as e:
            print(f"   âš ï¸ æ‹Ÿåˆå¤±è´¥ (degree={degree}): {e}")

    # 2. æ–°å¢ï¼šå¯¹æ•°æ‹Ÿåˆ (Logarithmic) y = a + b * ln(x)
    # è¿™ç§æ›²çº¿ç¬¦åˆ"å¢é•¿é€Ÿåº¦è¶Šæ¥è¶Šæ…¢ï¼Œè¶Šå¾€å³è¶Šå¹³"çš„ç‰¹å¾
    try:
        # è¿‡æ»¤æ‰ x <= 0 çš„ç‚¹ (logå®šä¹‰åŸŸ)
        valid_mask = x_points > 0
        x_log = np.log(x_points[valid_mask])
        y_valid = y_points[valid_mask]
        
        # çº¿æ€§æ‹Ÿåˆ: y = slope * log(x) + intercept
        slope, intercept = np.polyfit(x_log, y_valid, 1)
        
        # å®šä¹‰é¢„æµ‹å‡½æ•°
        def log_func(x):
            x = np.array(x)
            # é¿å… log(<=0)
            return slope * np.log(np.maximum(x, 1e-10)) + intercept
            
        y_pred_log = log_func(x_points)
        r2_log = r2_score(y_points, y_pred_log)
        
        results['log'] = {
            'coeffs': [slope, intercept],
            'poly': log_func,
            'r2': r2_log,
            'label': 'Logarithmic' # æ ‡è¯†ä¸ºå¯¹æ•°æ›²çº¿
        }
    except Exception as e:
        print(f"   âš ï¸ å¯¹æ•°æ‹Ÿåˆå¤±è´¥: {e}")
    
    return results
# ...existing code...
def plot_comparison_fits(
    x_max_points, y_max_points,
    x_max_points_filtered, y_max_points_filtered,
    outlier_indices,
    save_dir: str = "pii_leakage_analysis"
):
    """
    ç»˜åˆ¶å¤šç§æ‹Ÿåˆçš„å¯¹æ¯”å›¾ï¼ˆå¸¦å¼‚å¸¸å€¼æ ‡è®°ï¼‰
    """
    print(f"\nğŸ“Š æ‹Ÿåˆæ›²çº¿å¯¹æ¯”ï¼ˆæ’é™¤å¼‚å¸¸å€¼ï¼‰...")
    
    # è®¡ç®—æ‹Ÿåˆï¼š1, 2, 3é˜¶å¤šé¡¹å¼ + å¯¹æ•°æ‹Ÿåˆ
    fit_results = fit_and_evaluate_polynomials(
        x_max_points_filtered, y_max_points_filtered, 
        degrees=[1, 2, 3] 
    )
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    axes = axes.flatten()
    
    x_smooth = np.linspace(min(x_max_points_filtered), max(x_max_points_filtered), 200)
    colors = ['red', 'orange', 'green', 'purple']
    
    # è‡ªå®šä¹‰æ’åºï¼šæ•°å­—é”®æŒ‰å¤§å°æ’ï¼Œ'log' æ’æœ€å
    def sort_key(k):
        if k == 'log': return 100
        return k
    
    sorted_keys = sorted(list(fit_results.keys()), key=sort_key)
    
    for idx, key in enumerate(sorted_keys[:4]): # æœ€å¤šç”»4ä¸ª
        if idx >= len(axes): break
        
        result = fit_results[key]
        ax = axes[idx]
        
        # ç»˜åˆ¶æ­£å¸¸çš„ç‚¹
        ax.scatter(x_max_points_filtered, y_max_points_filtered, 
                  alpha=0.8, s=120, c='steelblue', 
                  edgecolors='darkblue', linewidth=2,
                  label='Normal points', zorder=3)
        
        # ç»˜åˆ¶å¼‚å¸¸å€¼
        if len(outlier_indices) > 0:
            x_outliers = [x_max_points[i] for i in outlier_indices]
            y_outliers = [y_max_points[i] for i in outlier_indices]
            ax.scatter(x_outliers, y_outliers, 
                      alpha=0.8, s=200, c='red', 
                      edgecolors='darkred', linewidth=2.5,
                      marker='x', label='Outliers', zorder=4)
        
        # ç»˜åˆ¶æ‹Ÿåˆæ›²çº¿
        y_smooth = result['poly'](x_smooth)
        ax.plot(x_smooth, y_smooth, 
               color=colors[idx], linewidth=3, 
               linestyle='--', alpha=0.9,
               label=result['label'])
        
        ax.set_xlabel('PIInum', fontsize=12, fontweight='bold')
        ax.set_ylabel('Max PL Value', fontsize=12, fontweight='bold')
        ax.set_title(f"{result['label']} (RÂ² = {result['r2']:.6f})", 
                    fontsize=14, fontweight='bold')
        
        ax.grid(True, alpha=0.3)
        ax.legend(fontsize=11, loc='lower right')
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, "polynomial_comparison_filtered.png")
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… æ‹Ÿåˆå¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
    
    # æ‰“å° RÂ² å¯¹æ¯”
    print("\nğŸ“ˆ RÂ² åˆ†æ•°å¯¹æ¯”:")
    print("-" * 60)
    for key in sorted_keys:
        r2 = fit_results[key]['r2']
        label = fit_results[key]['label']
        print(f"  {label}: RÂ² = {r2:.6f}")
    print("-" * 60)
    
    return fit_results
# ...existing code...


def plot_all_pl_scatter_with_best_fit(
    pii_counts: Dict[str, int],
    all_pl_values: List[float],
    all_sample_labels: List[int],
    label_to_name: Dict[int, str],
    fit_results: Dict,
    x_max_points, y_max_points,
    x_max_points_filtered, y_max_points_filtered,
    outlier_indices,
    save_dir: str = "pii_leakage_analysis",
    confidence_percentile: float = 95.0  # æ–°å¢å‚æ•°ï¼šç½®ä¿¡åº¦ç™¾åˆ†æ¯”
):
    """
    ç»˜åˆ¶æ‰€æœ‰ç‚¹ + æœ€ä½³æ‹Ÿåˆæ›²çº¿ + ä¸Šé™åŒ…ç»œçº¿ï¼ˆå¸¦å¼‚å¸¸å€¼æ ‡è®°ï¼‰
    """
    # è½¬æ¢ä¸º PIInum
    x_data = []
    y_data = []
    
    for i, label in enumerate(all_sample_labels):
        name = label_to_name.get(label, f"Unknown_{label}")
        if name in pii_counts:
            x_data.append(pii_counts[name])
            y_data.append(all_pl_values[i])
    
    if len(x_data) == 0:
        return
    
    # ================= ä¿®æ”¹å¼€å§‹ =================
    # å¼ºåˆ¶ä½¿ç”¨ Log æ‹Ÿåˆä½œä¸ºåŸºå‡†ï¼ˆåªè¦è®¡ç®—æˆåŠŸï¼‰ï¼Œå¿½ç•¥ R2 å¤§å°
    if 'log' in fit_results:
        best_key = 'log'
        print(f"\nğŸ”’ å·²å¼ºåˆ¶é”å®šä½¿ç”¨ Logarithmic æ›²çº¿ä½œä¸º Upper Bound åŸºå‡†")
    else:
        # å¦‚æœ Log æ‹Ÿåˆå¤±è´¥ï¼ˆä¾‹å¦‚æ•°æ®å…¨ä¸ºè´Ÿï¼‰ï¼Œæ‰å›é€€åˆ°è‡ªåŠ¨é€‰æ‹©
        best_key = max(fit_results.keys(), key=lambda k: fit_results[k]['r2'])
        print(f"\nâš ï¸ æœªæ‰¾åˆ° Log æ‹Ÿåˆç»“æœï¼Œå›é€€åˆ°æœ€ä½³ R2: {best_key}")
    # ================= ä¿®æ”¹ç»“æŸ =================

    best_result = fit_results[best_key]
    best_poly = best_result['poly']
    best_r2 = best_result['r2']
    best_label = best_result['label'] 
    
    print(f"ğŸ“Š ç»˜åˆ¶æ‹Ÿåˆå›¾: {best_label}")
    
    fig, ax = plt.subplots(figsize=(16, 10))
    
    
    # â‘  ç»˜åˆ¶æ‰€æœ‰æ•£ç‚¹ï¼ˆåŠé€æ˜ï¼‰
    ax.scatter(x_data, y_data, 
              alpha=0.15, s=15,
              c='lightblue', edgecolors='none',
              label=f'All PL values (n={len(x_data)})')
    
    # â‘¡ ç»˜åˆ¶æ­£å¸¸çš„ Max PL ç‚¹ï¼ˆè“è‰²ï¼‰
    ax.scatter(x_max_points_filtered, y_max_points_filtered,
              alpha=0.85, s=130,
              c='steelblue', edgecolors='darkblue', linewidth=2.5,
              marker='o', zorder=5,
              label=f'Normal Max PL (n={len(x_max_points_filtered)})')
    
    # â‘¢ ç»˜åˆ¶å¼‚å¸¸å€¼ç‚¹ï¼ˆçº¢è‰² Xï¼‰
    if len(outlier_indices) > 0:
        x_outliers = [x_max_points[i] for i in outlier_indices]
        y_outliers = [y_max_points[i] for i in outlier_indices]
        ax.scatter(x_outliers, y_outliers,
                  alpha=0.9, s=200,
                  c='red', edgecolors='darkred', linewidth=2.5,
                  marker='x', zorder=6,
                  label=f'Outliers (n={len(outlier_indices)})')
    
    # â‘£ ç»˜åˆ¶æœ€ä½³æ‹Ÿåˆæ›²çº¿ (Mean Trend)
    x_smooth = np.linspace(min(x_max_points_filtered), max(x_max_points_filtered), 300)
    y_smooth = best_poly(x_smooth)
    
    ax.plot(x_smooth, y_smooth, 
           color='darkgreen', linewidth=4, 
           linestyle='--', alpha=0.9,
           label=f'Best Fit: {best_label} (RÂ²={best_r2:.4f})')

    # ===================== ç»˜åˆ¶ä¸Šé™åŒ…ç»œçº¿ =====================
    # 1. è®¡ç®—æ®‹å·®
    y_pred_filtered = best_poly(x_max_points_filtered)
    residuals = y_max_points_filtered - y_pred_filtered
    
    # 2. è®¡ç®—åç§»é‡
    upper_shift = np.percentile(residuals, confidence_percentile)
    
    # 3. ç”Ÿæˆä¸Šé™æ›²çº¿ (åŸºå‡†æ›²çº¿ + åç§»é‡)
    # å¦‚æœåŸºå‡†æ˜¯ Log æ›²çº¿ï¼Œè¿™ä¸ªä¸Šé™æ›²çº¿ä¹Ÿæ˜¯ Log æ›²çº¿ï¼Œç¬¦åˆ"å¼¯å¼¯çš„å˜å¹³"çš„è¦æ±‚
    y_upper_bound = y_smooth + upper_shift
    
    
    # 4. ç»˜åˆ¶ä¸Šé™æ›²çº¿
    ax.plot(x_smooth, y_upper_bound, 
           color='crimson', linewidth=3, 
           linestyle='-.', alpha=0.8,
           label=f'Upper Bound ({confidence_percentile}% Confidence)')
    
    # 5. å¡«å……åŒºåŸŸ (å¯é€‰ï¼Œå¢åŠ è§†è§‰æ•ˆæœ)
    ax.fill_between(x_smooth, y_smooth, y_upper_bound, color='green', alpha=0.05)
    
    # 6. è®¡ç®—å®é™…è¦†ç›–ç‡ (æ£€æŸ¥æœ‰å¤šå°‘ Max PL ç‚¹åœ¨è¿™æ¡çº¿ä¸‹é¢)
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ£€æŸ¥æ‰€æœ‰çš„ Max PL ç‚¹ï¼ˆåŒ…æ‹¬å¼‚å¸¸å€¼ï¼‰
    y_pred_all_max = best_poly(x_max_points) + upper_shift
    covered_mask = y_max_points <= y_pred_all_max
    coverage_count = np.sum(covered_mask)
    coverage_pct = (coverage_count / len(y_max_points)) * 100
    
    print(f"   ğŸ“ˆ ä¸Šé™åŒ…ç»œçº¿ç»Ÿè®¡:")
    print(f"     - è®¾å®šç½®ä¿¡åº¦: {confidence_percentile}% (åŸºäºæ­£å¸¸ç‚¹)")
    print(f"     - å®é™…è¦†ç›–ç‡: {coverage_pct:.2f}% (æ‰€æœ‰ Max PL ç‚¹)")
    print(f"     - åç§»é‡: +{upper_shift:.4f}")
    # ==============================================================
    
    # â‘¤ è®¡ç®—ç›¸å…³ç³»æ•°
    corr_all = np.corrcoef(x_data, y_data)[0, 1]
    corr_normal = np.corrcoef(x_max_points_filtered, y_max_points_filtered)[0, 1]
    
    ax.set_xlabel('PIInum', fontsize=16, fontweight='bold')
    ax.set_ylabel('PL Value', fontsize=16, fontweight='bold')
    ax.set_title(f'All PL Values vs PIInum with Upper Bound Envelope\n'
                f'Trend RÂ²={best_r2:.4f} | Upper Bound Covers {coverage_pct:.1f}% of Max Points', 
                fontsize=18, fontweight='bold')
    
    # æ˜¾ç¤ºæ‹Ÿåˆæ–¹ç¨‹å’Œä¸Šé™æ–¹ç¨‹
    
    # æ˜¾ç¤ºæ‹Ÿåˆæ–¹ç¨‹
    if best_key == 'log':
        coeffs = best_result['coeffs']
        equation_text = (f"Trend: y = {coeffs[0]:.4f} * ln(x) + {coeffs[1]:.4f}\n"
                         f"Upper: y = Trend + {upper_shift:.4f}")
    else:
        equation_text = (f"Trend: {best_label}\n"
                         f"Upper: y = Trend + {upper_shift:.4f}")

    ax.text(0.02, 0.98, equation_text, 
           transform=ax.transAxes, fontsize=11,
           verticalalignment='top',
           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8, edgecolor='gray'),
           family='monospace')
    
    ax.grid(True, alpha=0.3)
    ax.legend(fontsize=13, loc='upper left', framealpha=0.95)
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, "all_pl_scatter_best_fit_filtered.png")
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… æœ€ä½³æ‹Ÿåˆå›¾ï¼ˆæ’é™¤å¼‚å¸¸å€¼ï¼‰å·²ä¿å­˜: {save_path}")
    print(f"   - æ€»ç‚¹æ•°: {len(x_data)}")
    print(f"   - æ­£å¸¸ Max PL ç‚¹æ•°: {len(x_max_points_filtered)}")
    print(f"   - å¼‚å¸¸å€¼ç‚¹æ•°: {len(outlier_indices)}")
    print(f"   - æ­£å¸¸ç‚¹ç›¸å…³ç³»æ•°: {corr_normal:.4f}")


def print_outlier_info(x_max_points, y_max_points, outlier_indices, label_to_name, pii_counts):
    """
    æ‰“å°å¼‚å¸¸å€¼çš„è¯¦ç»†ä¿¡æ¯
    """
    print("\n" + "=" * 80)
    print("ğŸ” å¼‚å¸¸å€¼è¯¦ç»†ä¿¡æ¯")
    print("=" * 80)
    
    for idx in sorted(outlier_indices):
        pii = x_max_points[idx]
        max_pl = y_max_points[idx]
        
        # æ‰¾å¯¹åº”çš„æ¼”å‘˜
        actor_name = None
        for name, pii_num in pii_counts.items():
            if pii_num == pii:
                actor_name = name
                break
        
        print(f"\nâš ï¸  å¼‚å¸¸å€¼ #{idx+1}")
        print(f"   æ¼”å‘˜: {actor_name or 'Unknown'}")
        print(f"   PIInum: {pii}")
        print(f"   Max PL: {max_pl:.4f}")


# ===================== è¾…åŠ©å‡½æ•° =====================

def load_pii_counts(data_file: str) -> Dict[str, int]:
    """åŠ è½½ PIInum"""
    df = pd.read_json(data_file)
    if 'name' not in df.columns or 'PIInum' not in df.columns:
        raise ValueError(f"æ•°æ®æ–‡ä»¶å¿…é¡»åŒ…å« 'name' å’Œ 'PIInum' åˆ—")
    return dict(zip(df['name'], df['PIInum']))


def aggregate_by_label_probs_mult(probs: np.ndarray, labels: np.ndarray, eps: float = 1e-12):
    """èšåˆä¸ºå¯¹è±¡çº§æ¦‚ç‡"""
    labels = labels.astype(int)
    uniq = np.unique(labels)
    P = np.clip(probs, eps, 1.0)
    agg_logp = []
    agg_labels = []
    for lb in uniq:
        block = P[labels == lb]
        logp = np.log(block).sum(axis=0)
        agg_logp.append(logp)
        agg_labels.append(lb)
    agg_logp = np.vstack(agg_logp)
    
    m = np.max(agg_logp, axis=1, keepdims=True)
    ex = np.exp(agg_logp - m)
    agg_probs = ex / ex.sum(axis=1, keepdims=True)
    return agg_probs, np.array(agg_labels)


# ===================== ä¸»å‡½æ•° =====================

def all_pl_comprehensive_analysis(
    data_file: str,
    prior_prob_file: str,
    posterior_prob_file: str,
    class_metric_file: str = "class_distance_matrix.json",
    output_dir: str = "pii_leakage_analysis",
    outlier_method: str = "iqr",  # "iqr", "zscore", æˆ– "modified_zscore"
    outlier_k: float = 1.5  # IQR å‚æ•°ï¼š1.5ï¼ˆæ ‡å‡†ï¼‰ã€3.0ï¼ˆä¸¥æ ¼ï¼‰
):
    """
    ç»˜åˆ¶æ‰€æœ‰ PL å€¼çš„å…³ç³»å›¾ï¼ˆæ”¯æŒå¼‚å¸¸å€¼æ’é™¤ï¼‰
    
    outlier_method: "iqr" (æ¨è), "zscore", "modified_zscore"
    outlier_k: IQR æ–¹æ³•çš„å‚æ•°ï¼Œ1.5=æ ‡å‡†ï¼Œ3.0=ä¸¥æ ¼
    """
    os.makedirs(output_dir, exist_ok=True)
    
    print("=" * 80)
    print(f"æ‰€æœ‰ PL å€¼ vs PIInum å…³ç³»åˆ†æï¼ˆå¼‚å¸¸å€¼æ’é™¤ï¼š{outlier_method}ï¼‰")
    print("=" * 80)
    
    # 1. åŠ è½½æ•°æ®
    print("\n1ï¸âƒ£  åŠ è½½ PIInum...")
    pii_counts = load_pii_counts(data_file)
    
    print("\n2ï¸âƒ£  åŠ è½½æ¦‚ç‡æ•°æ®...")
    with open(prior_prob_file, 'r', encoding='utf-8') as f:
        prior_data = json.load(f)
    with open(posterior_prob_file, 'r', encoding='utf-8') as f:
        posterior_data = json.load(f)
    
    print("\n3ï¸âƒ£  èšåˆä¸ºå¯¹è±¡çº§æ¦‚ç‡...")
    prior_probs, prior_labels = aggregate_by_label_probs_mult(
        np.array(prior_data['probs']), np.array(prior_data['labels']))
    posterior_probs, posterior_labels = aggregate_by_label_probs_mult(
        np.array(posterior_data['probs']), np.array(posterior_data['labels']))
    
    print("\n4ï¸âƒ£  å¯¹é½æ•°æ®...")
    labels = np.array([i for i in range(len(prior_probs)) if i < len(posterior_probs)])
    prior_probs = prior_probs[:len(labels)]
    posterior_probs = posterior_probs[:len(labels)]
    
    print("\n5ï¸âƒ£  åŠ è½½ç±»åˆ«è·ç¦»çŸ©é˜µ...")
    try:
        with open(class_metric_file, 'r', encoding='utf-8') as f:
            distance_data = json.load(f)
        class_metric = np.array(distance_data['distance_matrix'])
    except FileNotFoundError:
        class_metric = None
    
    print("\n6ï¸âƒ£  è®¡ç®—æ‰€æœ‰ç±»å¯¹çš„ PL å€¼...")
    all_pl_values, all_sample_labels = calculate_all_pl_pairs(
        prior_probs, posterior_probs, labels, class_metric)
    
    print("\n7ï¸âƒ£  æ„å»ºæ ‡ç­¾æ˜ å°„...")
    df = pd.read_json(data_file)
    if 'name' in df.columns:
        unique_names = sorted(df['name'].unique())
        label_to_name = {i: name for i, name in enumerate(unique_names)}
    else:
        label_to_name = {i: f"User_{i}" for i in range(len(labels))}
    
    print("\n8ï¸âƒ£  è®¡ç®—æ¯ä¸ª PIInum çš„ MAX PL...")
    x_data = []
    y_data = []
    for i, label in enumerate(all_sample_labels):
        name = label_to_name.get(label, f"Unknown_{label}")
        if name in pii_counts:
            x_data.append(pii_counts[name])
            y_data.append(all_pl_values[i])
    
    pii_to_max_pl = {}
    for x, y in zip(x_data, y_data):
        if x not in pii_to_max_pl:
            pii_to_max_pl[x] = y
        else:
            pii_to_max_pl[x] = max(pii_to_max_pl[x], y)
    
    x_max_points = np.array(sorted(list(pii_to_max_pl.keys())))
    y_max_points = np.array([pii_to_max_pl[x] for x in x_max_points])
    
    # ===== å…³é”®æ­¥éª¤ï¼šå¼‚å¸¸å€¼æ£€æµ‹ =====
    print(f"\n9ï¸âƒ£  æ£€æµ‹å¼‚å¸¸å€¼ï¼ˆæ–¹æ³•ï¼š{outlier_method}ï¼‰...")
    
    if outlier_method == "iqr":
        outliers, lower, upper = detect_outliers_iqr(y_max_points, k=outlier_k)
        print(f"   IQR èŒƒå›´: [{lower:.4f}, {upper:.4f}]")
    elif outlier_method == "zscore":
        outliers = detect_outliers_zscore(y_max_points, threshold=3)
    elif outlier_method == "modified_zscore":
        outliers = detect_outliers_modified_zscore(y_max_points, threshold=3.5)
    else:
        raise ValueError(f"æœªçŸ¥çš„å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•: {outlier_method}")
    
    outlier_indices = np.where(outliers)[0]
    print(f"   æ£€æµ‹åˆ°å¼‚å¸¸å€¼: {len(outlier_indices)} ä¸ª")
    
    if len(outlier_indices) > 0:
        print(f"   å¼‚å¸¸å€¼ç´¢å¼•: {list(outlier_indices)}")
    
    # è¿‡æ»¤æ•°æ®
    x_max_points_filtered = x_max_points[~outliers]
    y_max_points_filtered = y_max_points[~outliers]
    
    print(f"   è¿‡æ»¤åç‚¹æ•°: {len(x_max_points_filtered)} ä¸ª")
    
    # æ‰“å°å¼‚å¸¸å€¼ä¿¡æ¯
    print_outlier_info(x_max_points, y_max_points, outlier_indices, label_to_name, pii_counts)
    
    # æ‹Ÿåˆå¤šé¡¹å¼
    print("\nğŸ”Ÿ å¤šé¡¹å¼æ‹Ÿåˆå¯¹æ¯”...")
    fit_results = plot_comparison_fits(
        x_max_points, y_max_points,
        x_max_points_filtered, y_max_points_filtered,
        outlier_indices,
        output_dir
    )
    
    # ç»˜åˆ¶æœ€ä½³æ‹Ÿåˆ
    print("\n1ï¸âƒ£1ï¸âƒ£ ç»˜åˆ¶æœ€ä½³æ‹Ÿåˆå›¾...")
    plot_all_pl_scatter_with_best_fit(
        pii_counts, all_pl_values, all_sample_labels, 
        label_to_name, fit_results,
        x_max_points, y_max_points,
        x_max_points_filtered, y_max_points_filtered,
        outlier_indices,
        output_dir,
        confidence_percentile=95.0  # 95% ç½®ä¿¡åº¦åŒ…ç»œçº¿
    )
    
    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰åˆ†æå®Œæˆ!")
    print("=" * 80)


if __name__ == "__main__":
    data_file = r"C:\Users\phdwf\OneDrive\Desktop\textreidentify\TextReIdentification\data\WikiActors_50_masked_cleaned.json"
    prior_prob_file = "budget_2.0_independent_masked_abstract.json"
    posterior_prob_file = "budget_2.0_independent_noise_abstract.json"
    
    all_pl_comprehensive_analysis(
        data_file=data_file,
        prior_prob_file=prior_prob_file,
        posterior_prob_file=posterior_prob_file,
        class_metric_file="noise_2.0_independent_distance_matrix.json",
        output_dir="pii_leakage_analysis",

        outlier_method="iqr",      # "iqr", "zscore", "modified_zscore"
        outlier_k=1.5              # IQR å‚æ•°ï¼š1.5ï¼ˆæ ‡å‡†ï¼‰ã€3.0ï¼ˆä¸¥æ ¼ï¼‰
    )