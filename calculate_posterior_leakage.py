# -*- coding: utf-8 -*-
import json
import numpy as np
import matplotlib.pyplot as plt
import os
from typing import Dict, Any, List


# ===================== èšåˆä¸å¯¹é½ =====================
def extract_and_compute_class_metric(tri_model_path: str, num_classes: int):
    """
    ä» TRI æ¨¡å‹æå–ç±»åˆ«åµŒå…¥å¹¶è®¡ç®—è·ç¦»çŸ©é˜µ
    """
    from transformers import AutoModelForSequenceClassification
    from sklearn.metrics.pairwise import cosine_distances
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForSequenceClassification.from_pretrained(tri_model_path)
    
    # æå–åˆ†ç±»å™¨æƒé‡
    if hasattr(model, 'classifier'):
        if hasattr(model.classifier, 'weight'):
            class_embeddings = model.classifier.weight.detach().cpu().numpy()
        elif hasattr(model.classifier, 'out_proj'):
            class_embeddings = model.classifier.out_proj.weight.detach().cpu().numpy()
    else:
        raise AttributeError("Cannot find classifier in model")
    
    # è®¡ç®—ä½™å¼¦è·ç¦»
    distance_matrix = cosine_distances(class_embeddings)
    np.fill_diagonal(distance_matrix, 1e-12)
    
    print(f"âœ… ä»æ¨¡å‹æå–ç±»åˆ«è·ç¦»çŸ©é˜µ: {distance_matrix.shape}")
    return distance_matrix

def aggregate_by_label_logits(logits: np.ndarray, labels: np.ndarray):
    """
    å°†åŒä¸€ label çš„å¤šä¸ªçª—å£çº§ logits ç›¸åŠ ï¼ˆâ‰ˆç‹¬ç«‹è¯æ®ç›¸ä¹˜ï¼‰ï¼Œ
    å† softmax å¾—åˆ°â€œå¯¹è±¡çº§â€æ¦‚ç‡åˆ†å¸ƒã€‚
    è¿”å›ï¼šagg_probs (num_labels, C), agg_labels (num_labels,)
    """
    labels = labels.astype(int)
    uniq = np.unique(labels)
    agg_logits = []
    agg_labels = []
    for lb in uniq:
        L = logits[labels == lb]          # è¯¥äººçš„æ‰€æœ‰çª—å£ (k, C)
        summed = L.sum(axis=0)            # logits ç›¸åŠ 
        agg_logits.append(summed)
        agg_labels.append(lb)
    agg_logits = np.vstack(agg_logits)    # (num_labels, C)

    # æ•°å€¼ç¨³å®š softmax
    m = np.max(agg_logits, axis=1, keepdims=True)
    ex = np.exp(agg_logits - m)
    probs = ex / ex.sum(axis=1, keepdims=True)
    return probs, np.array(agg_labels)


def aggregate_by_label_probs_mult(probs: np.ndarray, labels: np.ndarray, eps: float = 1e-12):
    """
    å¤‡é€‰ï¼šå½“æ²¡æœ‰ logits æ—¶ï¼ŒæŠŠåŒä¸€ label çš„çª—å£çº§æ¦‚ç‡ç›¸ä¹˜ï¼ˆlog æ¦‚ç‡ç›¸åŠ ï¼‰å†å½’ä¸€åŒ–ã€‚
    è¿”å›ï¼šagg_probs (num_labels, C), agg_labels (num_labels,)
    """
    labels = labels.astype(int)
    uniq = np.unique(labels)
    P = np.clip(probs, eps, 1.0)
    agg_logp = []
    agg_labels = []
    for lb in uniq:
        block = P[labels == lb]           # (k, C)
        logp = np.log(block).sum(axis=0)  # æ¦‚ç‡ä¹˜ç§¯ â†’ log æ¦‚ç‡ç›¸åŠ 
        agg_logp.append(logp)
        agg_labels.append(lb)
    agg_logp = np.vstack(agg_logp)

    # log-sum-exp å½’ä¸€åŒ–
    m = np.max(agg_logp, axis=1, keepdims=True)
    ex = np.exp(agg_logp - m)
    agg_probs = ex / ex.sum(axis=1, keepdims=True)
    return agg_probs, np.array(agg_labels)


def align_by_label_after_aggregation(
    prior_probs: np.ndarray, prior_labels: np.ndarray,
    posterior_probs: np.ndarray, posterior_labels: np.ndarray
):
    """
    å‡è®¾ä¸¤ä¾§éƒ½å·²æŒ‰ label èšåˆä¸ºå¯¹è±¡çº§åˆ†å¸ƒã€‚
    å¯¹å…±åŒçš„ label æ’åºåå¯¹é½ï¼Œä¿è¯åŒä¸€ label åœ¨ A/B åŒä¸€è¡Œã€‚
    """
    common = np.intersect1d(np.unique(prior_labels), np.unique(posterior_labels))
    prior_idx = {lb: i for i, lb in enumerate(prior_labels)}
    post_idx  = {lb: i for i, lb in enumerate(posterior_labels)}
    A, B, L = [], [], []
    for lb in sorted(common):
        A.append(prior_probs[prior_idx[lb]])
        B.append(posterior_probs[post_idx[lb]])
        L.append(lb)
    return np.vstack(A), np.vstack(B), np.array(L)


# ===================== Pairwise mPL è®¡ç®— =====================

def calculate_pairwise_posterior_leakage(
    prior_probs: np.ndarray,      # shape: (N, C)  â€”â€” A
    posterior_probs: np.ndarray,  # shape: (N, C)  â€”â€” B
    class_metric: np.ndarray = None,  # shape: (C, C), d_{i,j}; ä¸æä¾›åˆ™å…¨ä¸º1
    epsilon: float = 1e-12
) -> Dict[str, Any]:
    """
    é€å¯¹ (i,j) ç±»åˆ«è®¡ç®— | log(B_i/B_j) - log(A_i/A_j) | / d_{i,j}
    è¿”å›æ‰€æœ‰ pairwise PL å€¼çš„é›†åˆï¼Œè€Œä¸æ˜¯ per-sample å¹³å‡ã€‚
    """
    assert prior_probs.shape == posterior_probs.shape, "prior/posterior ç»´åº¦ä¸ä¸€è‡´"
    N, C = prior_probs.shape

    # æ•°å€¼ç¨³å®š + è¡Œå½’ä¸€åŒ–
    A = np.clip(prior_probs, epsilon, 1.0)
    B = np.clip(posterior_probs, epsilon, 1.0)
    A = A / A.sum(axis=1, keepdims=True)
    B = B / B.sum(axis=1, keepdims=True)

    # ç±»é—´åº¦é‡
    if class_metric is None:
        D = np.ones((C, C), dtype=float)
        np.fill_diagonal(D, np.inf)  # i==j ä¸è®¡
    else:
        D = np.array(class_metric, dtype=float)
        assert D.shape == (C, C)
        D = np.where(D <= 0, epsilon, D)

    pairwise_pl_values = []

    for s in range(N):
        la = np.log(A[s])   # (C,)
        lb = np.log(B[s])   # (C,)
        # é€å¯¹å·®å€¼ï¼š[lb_i - lb_j] - [la_i - la_j]
        diff = np.abs((lb[:, None] - lb[None, :]) - (la[:, None] - la[None, :]))  # (C,C)
        mask = ~np.eye(C, dtype=bool)
        normed = diff[mask] / D[mask]
        pairwise_pl_values.extend(normed.tolist())

    arr = np.array(pairwise_pl_values)
    stats = {
        'mean_pl': float(np.mean(arr)) if arr.size else 0.0,
        'std_pl': float(np.std(arr)) if arr.size else 0.0,
        'min_pl': float(np.min(arr)) if arr.size else 0.0,
        'max_pl': float(np.max(arr)) if arr.size else 0.0,
        'median_pl': float(np.median(arr)) if arr.size else 0.0,
        'total_counts': int(arr.size)
    }
    return {'pairwise_pl': pairwise_pl_values, 'statistics': stats}


# ===================== å¯è§†åŒ–ï¼ˆpairwise ç‰ˆæœ¬ï¼‰ =====================

def create_pl_distribution_plot(
    pl_values: List[float],
    save_path: str,
    dataset_comparison: str,
    violation_threshold: float = 3.0
):
    """æŠŠæ¯ä¸ª (i,j) çš„ PL å½“ä½œä¸€ä¸ªæ ·æœ¬/è®¡æ•°æ¥ç”»ç›´æ–¹å›¾"""
    arr = np.array(pl_values)
    if len(arr) == 0:
        print("No PL values to plot")
        return

    plt.figure(figsize=(12, 8))

    counts, bin_edges = np.histogram(arr, bins=50)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    bin_width = bin_edges[1] - bin_edges[0]

    not_violated_mask = bin_centers <= violation_threshold
    violated_mask = bin_centers > violation_threshold

    plt.bar(bin_centers[not_violated_mask], counts[not_violated_mask],
            width=bin_width, alpha=0.7, edgecolor='black',
            color='steelblue', label='Not violated')

    if np.any(violated_mask):
        plt.bar(bin_centers[violated_mask], counts[violated_mask],
                width=bin_width, alpha=0.7, edgecolor='black',
                color='red', label='Violated')

    plt.axvline(violation_threshold, color='darkred', linestyle='-', linewidth=3,
                label=f'Violation threshold (Îµ = {violation_threshold})')
    plt.axvline(np.mean(arr), color='orange', linestyle='--', linewidth=2,
                label=f'Mean: {np.mean(arr):.4f}')

    violated_values = arr[arr > violation_threshold]
    violation_ratio = len(violated_values) / len(arr) * 100 if len(arr) > 0 else 0.0

    plt.xlabel('Pairwise Posterior Leakage (PL)', fontsize=12)
    plt.ylabel('Count (number of class pairs)', fontsize=12)
    plt.title(f'Posterior Leakage Distribution (pairwise)\n{dataset_comparison}\nViolation ratio: {violation_ratio:.1f}%',
              fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… Pairwise PL distribution plot saved: {save_path}")


# ===================== ä¸»æµç¨‹ =====================

def analyze_posterior_leakage_between_datasets(
    prior_file: str,
    posterior_file: str,
    save_dir: str = "posterior_leakage_results",
    class_metric: np.ndarray = None,   # å¯é€‰ï¼šä¼ å…¥ (C,C) çš„ç±»é—´è·ç¦»çŸ©é˜µ
    violation_threshold: float = 3.0
):
    """
    åˆ†æä¸¤ä¸ªæ•°æ®é›†ä¹‹é—´çš„åéªŒæ¦‚ç‡æ³„éœ²ï¼ˆpairwise ç»Ÿè®¡ï¼‰ï¼š
    - çª—å£çº§ -> ï¼ˆæŒ‰ label èšåˆï¼‰å¯¹è±¡çº§æ¦‚ç‡
    - å¯¹åŒä¸€å¯¹è±¡çš„æ‰€æœ‰ç±»å¯¹ (i,j) äº§ç”Ÿä¸€ä¸ª PL å€¼ï¼ˆcount=1ï¼‰
    - ç›´æ–¹å›¾æŒ‰æ‰€æœ‰ pairwise PL å€¼ä½œå›¾
    """
    print(f"\n{'='*60}")
    print(f"åˆ†æåéªŒæ³„éœ²ï¼ˆpairwise æˆå¯¹èµ”ç‡å®šä¹‰ï¼‰")
    print(f"Prior (æœªåŠ å™ª): {prior_file}")
    print(f"Posterior (åŠ å™ª): {posterior_file}")
    print(f"{'='*60}\n")

    # åŠ è½½æ•°æ®
    with open(prior_file, 'r', encoding='utf-8') as f:
        prior_data = json.load(f)
    with open(posterior_file, 'r', encoding='utf-8') as f:
        posterior_data = json.load(f)

    prior_labels_raw = np.array(prior_data['labels'])
    posterior_labels_raw = np.array(posterior_data['labels'])

    # --- ä¼˜å…ˆä½¿ç”¨ logits åšâ€œè”åˆè§‚æµ‹â€èšåˆï¼›æ—  logits æ—¶é€€åŒ–ä¸ºæ¦‚ç‡ä¹˜ç§¯èšåˆ ---
    if 'logits' in prior_data and 'logits' in posterior_data:
        prior_logits_raw = np.array(prior_data['logits'])
        posterior_logits_raw = np.array(posterior_data['logits'])

        prior_probs_agg, prior_labels_agg = aggregate_by_label_logits(prior_logits_raw, prior_labels_raw)
        posterior_probs_agg, posterior_labels_agg = aggregate_by_label_logits(posterior_logits_raw, posterior_labels_raw)
        print("å·²ä½¿ç”¨ logits èšåˆä¸ºå¯¹è±¡çº§æ¦‚ç‡ï¼ˆè”åˆè§‚æµ‹ï¼‰ã€‚")
    else:
        prior_probs_raw = np.array(prior_data['probs'])
        posterior_probs_raw = np.array(posterior_data['probs'])

        prior_probs_agg, prior_labels_agg = aggregate_by_label_probs_mult(prior_probs_raw, prior_labels_raw)
        posterior_probs_agg, posterior_labels_agg = aggregate_by_label_probs_mult(posterior_probs_raw, posterior_labels_raw)
        print("æœªå‘ç° logitsï¼Œå·²ä½¿ç”¨æ¦‚ç‡ä¹˜ç§¯è¿‘ä¼¼èšåˆä¸ºå¯¹è±¡çº§æ¦‚ç‡ã€‚")

    # --- å¯¹é½ï¼ˆå¯¹è±¡çº§ï¼‰---
    prior_probs, posterior_probs, labels = align_by_label_after_aggregation(
        prior_probs_agg, prior_labels_agg,
        posterior_probs_agg, posterior_labels_agg
    )

    print(f"å¯¹é½åå¯¹è±¡æ•°ï¼š{len(labels)}")
    print(f"ç±»åˆ«æ•°ï¼š{prior_probs.shape[1]}")

    # è®¡ç®— pairwise åéªŒæ³„éœ²
    print("è®¡ç®— pairwise åéªŒæ³„éœ²ï¼ˆåŒä¸€å¯¹è±¡å†…æˆå¯¹ç±»èµ”ç‡å˜åŒ–ï¼‰...")
    pl_result = calculate_pairwise_posterior_leakage(
        prior_probs, posterior_probs,
        class_metric=class_metric
    )

    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)

    # æå–æ•°æ®é›†åç§°
    prior_name = os.path.basename(prior_file).replace('per_sample_probs_', '').replace('.json', '')
    posterior_name = os.path.basename(posterior_file).replace('per_sample_probs_', '').replace('.json', '')
    comparison_name = f"{prior_name}_vs_{posterior_name}"

    # ä¿å­˜è¯¦ç»†ç»“æœ
    detailed_result = {
        'comparison': {
            'prior_dataset': prior_name,
            'posterior_dataset': posterior_name,
            'num_aligned_objects': int(len(labels)),
            'num_classes': int(prior_probs.shape[1]),
            'aggregation': 'logits_sum_softmax' if 'logits' in prior_data and 'logits' in posterior_data else 'prob_product_norm',
            'pairwise_total_counts': pl_result['statistics']['total_counts']
        },
        'pairwise_posterior_leakage': {
            'pairwise_pl': pl_result['pairwise_pl'],
            'statistics': pl_result['statistics']
        }
    }

    detailed_path = os.path.join(save_dir, f"{comparison_name}_pairwise_leakage_detailed.json")
    with open(detailed_path, 'w', encoding='utf-8') as f:
        json.dump(detailed_result, f, indent=2, ensure_ascii=False)
    print(f"âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜: {detailed_path}")

    # åˆ›å»ºåˆ†å¸ƒå›¾ï¼ˆpairwiseï¼‰
    distribution_path = os.path.join(save_dir, f"{comparison_name}_pairwise_distribution.png")
    create_pl_distribution_plot(
        pl_result['pairwise_pl'],
        distribution_path,
        f"{prior_name} (prior) vs {posterior_name} (posterior)",
        violation_threshold=violation_threshold
    )

    # ç”Ÿæˆæ‘˜è¦
    stats = pl_result['statistics']
    summary_path = os.path.join(save_dir, f"{comparison_name}_pairwise_summary.txt")
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write(f"Posterior Leakage Analysis Summary (Pairwise mPL)\n")
        f.write("=" * 50 + "\n")
        f.write(f"Prior Dataset: {prior_name}\n")
        f.write(f"Posterior Dataset: {posterior_name}\n")
        f.write(f"Aligned Objects: {len(labels)}\n")
        f.write(f"Number of Classes: {prior_probs.shape[1]}\n")
        f.write(f"Aggregation: {'logits_sum_softmax' if 'logits' in prior_data and 'logits' in posterior_data else 'prob_product_norm'}\n\n")

        f.write("Pairwise Posterior Leakage Statistics:\n")
        f.write("-" * 30 + "\n")
        f.write(f"Total Counts (pairs across all objects): {stats['total_counts']}\n")
        f.write(f"Mean PL: {stats['mean_pl']:.6f}\n")
        f.write(f"Std PL: {stats['std_pl']:.6f}\n")
        f.write(f"Median PL: {stats['median_pl']:.6f}\n")
        f.write(f"Min PL: {stats['min_pl']:.6f}\n")
        f.write(f"Max PL: {stats['max_pl']:.6f}\n")

    print(f"âœ… æ‘˜è¦å·²ä¿å­˜: {summary_path}")

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*60}")
    print("åéªŒæ³„éœ²ç»Ÿè®¡ (Pairwise mPL):")
    print(f"{'='*60}")
    print(f"Counts: {stats['total_counts']}")
    print(f"å¹³å‡ PL: {stats['mean_pl']:.6f}")
    print(f"æ ‡å‡†å·® PL: {stats['std_pl']:.6f}")
    print(f"ä¸­ä½æ•° PL: {stats['median_pl']:.6f}")
    print(f"æœ€å° PL: {stats['min_pl']:.6f}")
    print(f"æœ€å¤§ PL: {stats['max_pl']:.6f}")
    print(f"{'='*60}\n")

    return detailed_result


# ===================== CLI =====================

if __name__ == "__main__":
    # å¯é…ç½®é¡¹ï¼šä¿®æ”¹é¢„ç®—ã€ç­–ç•¥ä¸é˜ˆå€¼
    BUDGET = 0.0
    STRATEGY = "independent"

    try:
        budget_tag = f"{float(BUDGET):.1f}"
    except Exception:
        budget_tag = str(BUDGET)
    # æ„å»ºæ–‡ä»¶å
    prior_file = f"budget_{budget_tag}_{STRATEGY}_original_abstract.json"
    posterior_file = f"budget_{budget_tag}_{STRATEGY}_noise_abstract.json"
    distance_fname = f"noise_{budget_tag}_{STRATEGY}_distance_matrix.json"

    files = [prior_file, posterior_file]

    # åŠ è½½ç±»åˆ«è·ç¦»çŸ©é˜µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    try:
        with open(distance_fname, "r", encoding="utf-8") as f:
            distance_data = json.load(f)
        class_metric = np.array(distance_data['distance_matrix'])
        print(f"âœ… åŠ è½½ç±»åˆ«è·ç¦»çŸ©é˜µ: {class_metric.shape} ({distance_fname})")
    except FileNotFoundError:
        print(f"âš ï¸ æœªæ‰¾åˆ°ç±»åˆ«è·ç¦»çŸ©é˜µ: {distance_fname}ï¼Œä½¿ç”¨é»˜è®¤å€¼ (å…¨1)")
        class_metric = None

    analyze_posterior_leakage_between_datasets(
        prior_file=files[0],
        posterior_file=files[1],
        save_dir="posterior_leakage_results",
        class_metric=class_metric,
        violation_threshold=BUDGET
    )

    print("\nğŸ‰ åˆ†æå®Œæˆ!")